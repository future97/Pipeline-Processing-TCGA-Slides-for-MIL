{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab698c9",
   "metadata": {},
   "source": [
    "# S04: Extracting Patch Features\n",
    "\n",
    "Here we utilize a image feature extractor to extract deep features from all the patches that we have obtained from the step `S03`. The image feature extractor used in this step is `CONCH` (https://github.com/mahmoodlab/CONCH), a vision-language model pretrained on pathology images. \n",
    "\n",
    "We also adopt `CLAM` (https://github.com/mahmoodlab/CLAM) for the above purpose. \n",
    "\n",
    "## 1. My notes\n",
    "\n",
    "In this step, for each slide, its patch coordinates, which are stored in a `h5` file (see `ROOT_DIR_FOR_DATA_SAVING/tiles-l1-s256/patches/` in your server), would be loaded and then used to locate certain patch regions (each with the size you specified in the step `S03`) in this slide image (at the magnification you specified in the step `S03`). Meanwhile, the source file of the slide will also be loaded for reading patch regions. At the end, all patch features of the slide will be saved in a `pt` file. \n",
    "\n",
    "Key codes are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e91dcb1",
   "metadata": {},
   "source": [
    "```python\n",
    "# key codes for calculating patch features\n",
    "all_feats = None\n",
    "all_coors = None\n",
    "for count, (batch, coords) in enumerate(loader):\n",
    "    coords = torch.from_numpy(coords)\n",
    "    with torch.no_grad():   \n",
    "        if count % print_every == 0:\n",
    "            print('batch {}/{}, {} files processed'.format(count, len(loader), count * batch_size))\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        mini_bs = coords.shape[0]\n",
    "\n",
    "        features = model(batch).cpu()\n",
    "        if all_feats is None:\n",
    "            all_feats = features\n",
    "            all_coors = coords\n",
    "        else:\n",
    "            all_feats = torch.cat([all_feats, features], axis=0)\n",
    "            all_coors = torch.cat([all_coors, coords], axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f49349",
   "metadata": {},
   "source": [
    "## 2. Running feature extraction\n",
    "\n",
    "Similarly, there are also two options for you. The first one, official CLAM, is recommended. The second one, our improved CLAM, would requires extra learning costs.\n",
    "\n",
    "### 2.1 Using official CLAM\n",
    "\n",
    "Please refer to https://github.com/mahmoodlab/CLAM for the details of patch feature extraction.\n",
    "\n",
    "### 2.2 Using an improved version of CLAM\n",
    "\n",
    "For this step, we have improved CLAM specifically in terms of \n",
    "- more alternative architectures (including 6+ pretrained SOTA model) for patch feature extracting, where \n",
    "  - `CONCH` is recommended \n",
    "  - `CTransPath` and `PLIP` are good alternatives (both are free for use), when you cannot use CONCH due to limited access rights\n",
    "  - `CTransPath` and `PLIP` scripts are provided in [tools/scripts](https://github.com/liupei101/Pipeline-Processing-TCGA-Slides-for-MIL/tree/main/tools/scripts)\n",
    "  - The `truncated ResNet50` and `ResNet18 w/ SimCL` are **NOT** recommended\n",
    "\n",
    "A detailed bash script (placed at `./tools/scripts/S04-Extracting-Feats.sh`), with `CONCH` as the patch feature extractor, is as follows:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "set -e\n",
    "\n",
    "# Sample patches of SIZE x SIZE at LEVEL \n",
    "LEVEL=1\n",
    "SIZE=256\n",
    "\n",
    "# Path where CLAM is installed\n",
    "DIR_REPO=../CLAM\n",
    "\n",
    "# Root path to pathology images \n",
    "DIR_RAW_DATA=/NAS02/RawData/tcga_rcc\n",
    "DIR_EXP_DATA=/NAS02/ExpData/tcga_rcc\n",
    "\n",
    "# Sub-directory to the patch coordinates generated from S03\n",
    "SUBDIR_READ=tiles-l${LEVEL}-s${SIZE}\n",
    "\n",
    "# Arch to be used for patch feature extraction (CONCH is strongly recommended)\n",
    "ARCH=CONCH\n",
    "\n",
    "# Model path\n",
    "# You need to first apply for its access rights via https://huggingface.co/MahmoodLab/CONCH\n",
    "# and then download a model file named `pytorch_model.bin`.\n",
    "MODEL_CKPT=/path/to/conch/pytorch_model.bin\n",
    "\n",
    "# Sub-directory to the patch features \n",
    "SUBDIR_SAVE=feats-l${LEVEL}-s${SIZE}-${ARCH}\n",
    "\n",
    "cd ${DIR_REPO}\n",
    "\n",
    "echo \"running for extracting features from all tiles\"\n",
    "CUDA_VISIBLE_DEVICES=0 python3 extract_features_fp.py \\\n",
    "    --arch ${ARCH} \\\n",
    "    --ckpt_path ${MODEL_CKPT} \\\n",
    "    --data_h5_dir ${DIR_EXP_DATA}/${SUBDIR_READ} \\\n",
    "    --data_slide_dir ${DIR_RAW_DATA} \\\n",
    "    --csv_path ${DIR_EXP_DATA}/${SUBDIR_READ}/process_list_autogen.csv \\\n",
    "    --feat_dir ${DIR_EXP_DATA}/${SUBDIR_SAVE} \\\n",
    "    --batch_size 128 \\\n",
    "    --slide_ext .svs \\\n",
    "    --slide_in_child_dir \\\n",
    "    --proj_to_contrast N\n",
    "```\n",
    "\n",
    "You could run this script using the following command:\n",
    "```bash\n",
    "nohup ./S04-Extracting-Feats.sh > S04-Extract-Feats.log 2>&1 &\n",
    "```\n",
    "\n",
    "Full running logs could be found in `./tools/scripts/S04-Extract-Feats.log`. \n",
    "\n",
    "Next, we check if the number of generated files is consistent with that of patch files from the step `S03`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf2d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "DIR_FEAT = \"/NAS02/ExpData/tcga_rcc/feats-l1-s256-CONCH/pt_files\"\n",
    "feat_files = [f for f in os.listdir(DIR_FEAT) if f.endswith(\".pt\")]\n",
    "print(\"This step generated {} feature files in {}.\".format(len(feat_files), DIR_FEAT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f48269",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATCH = \"/NAS02/ExpData/tcga_rcc/tiles-l1-s256/patches\"\n",
    "patch_files = [f for f in os.listdir(DIR_PATCH) if f.endswith(\".h5\")]\n",
    "print(\"The step S03 generated {} patch files in {}.\".format(len(patch_files), DIR_PATCH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_filenames = [osp.splitext(f)[0] for f in feat_files]\n",
    "patch_filenames = [osp.splitext(f)[0] for f in patch_files]\n",
    "flag = False\n",
    "for f in patch_filenames:\n",
    "    if f not in feat_filenames:\n",
    "        flag = True\n",
    "        print(\"Expected {}, but it was not found in features files.\".format(f))\n",
    "if flag:\n",
    "    print(\"Some slides were not processed.\")\n",
    "else:\n",
    "    print(\"All slides in patch directory have been processed in this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ebd48",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 An example\n",
    "\n",
    "The running log of the first WSI is presented as follows:\n",
    "\n",
    "```txt\n",
    "progress: 0/940\n",
    "TCGA-2K-A9WE-01Z-00-DX1.ED8ADE3B-D49B-403B-B4EB-BD11D91DD676\n",
    "downsample [4.00005125 4.00008641]\n",
    "downsampled_level_dim [39021 23146]\n",
    "level_dim [39021 23146]\n",
    "name TCGA-2K-A9WE-01Z-00-DX1.ED8ADE3B-D49B-403B-B4EB-BD11D91DD676\n",
    "patch_level 1\n",
    "patch_size 256\n",
    "save_path /NAS02/ExpData/tcga_rcc/tiles-l1-s256/patches\n",
    "\n",
    "feature extraction settings:\n",
    "-- target patch size:  None\n",
    "-- imagenet_pretrained:  False\n",
    "-- patches sampler: None\n",
    "-- color normalization: None\n",
    "-- color argmentation: None\n",
    "-- add_patch_noise: None\n",
    "-- vertical_flip: False\n",
    "-- transformations:  Compose(\n",
    "    Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)\n",
    "    CenterCrop(size=(256, 256))\n",
    "    <function _convert_to_rgb at 0x7f63c5177160>\n",
    "    ToTensor()\n",
    "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    ")\n",
    "-- enable direct transform:  True\n",
    "processing /NAS02/ExpData/tcga_rcc/tiles-l1-s256/patches/TCGA-2K-A9WE-01Z-00-DX1.ED8ADE3B-D49B-403B-B4EB-BD11D91DD676.h5: total of 57 batches\n",
    "batch 0/57, 0 files processed\n",
    "batch 20/57, 2560 files processed\n",
    "batch 40/57, 5120 files processed\n",
    "features size:  torch.Size([7274, 1024])\n",
    "saved pt file:  /NAS02/ExpData/tcga_rcc/feats-l1-s256-CONCH/pt_files/TCGA-2K-A9WE-01Z-00-DX1.ED8ADE3B-D49B-403B-B4EB-BD11D91DD676.pt\n",
    "\n",
    "computing features for /NAS02/ExpData/tcga_rcc/feats-l1-s256-CONCH/pt_files/TCGA-2K-A9WE-01Z-00-DX1.ED8ADE3B-D49B-403B-B4EB-BD11D91DD676.pt took 75.39572024345398 s\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
